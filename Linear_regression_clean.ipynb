{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOl/aMFpohxjBPGIqciNEDL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nedokormysh/Stepik_algorithms_ml_course/blob/linear_regression/Linear_regression_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4keW28HJlzVv"
      },
      "outputs": [],
      "source": [
        "# correct version\n",
        "\n",
        "from typing import Union, Callable\n",
        "\n",
        "class MyLineReg():\n",
        "    \"\"\"\n",
        "    Линейная регрессия\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_iter : int, optional\n",
        "        Количество шагов градиентного спуска, by default 100\n",
        "    learning_rate : Union[float, Callable], optional\n",
        "        Коэффициент скорости обучения градиентного спуска.\n",
        "        Либо фиксированный шаг обучения, если на вход пришло значение float.\n",
        "        Либо шаг обучения изменяется в соответствии с функцией расчёта, если на вход пришла lambda-функция, by default 0.1\n",
        "    weights : np.ndarray, optional\n",
        "        Веса модели, by default None\n",
        "    metric : str, optional\n",
        "        Метрика, вычислается отдельно от функциии потерь.\n",
        "        Принимает одно из следующих значений: mae, mse, rmse, mape, r2, by default None\n",
        "    reg : str, optional\n",
        "        Вид регуляризации. Принимает одно из следующих значений: l1, l2, elasticnet, by default None\n",
        "    l1_coef : float, optional\n",
        "        Коэффициент L1 регуляризации. Принимает значения от 0.0 до 1.0, by default 0\n",
        "    l2_coef : float, optional\n",
        "        Коэффициент L2 регуляризации. Принимает значения от 0.0 до 1.0, by default 0\n",
        "    sgd_sample : Union[int, float], optional\n",
        "        Количество примеров, которое будет использоваться на каждой итерации обучения.\n",
        "        Может принимать целые числа - тогда используем точное количество примеров, либо дробные от 0.0 до 1.0 - тогда выбираем процент примеров от общего количества, by default None\n",
        "    random_state : int, optional\n",
        "        Рандомное состояние, by default 42\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 n_iter: int = 100,\n",
        "                 learning_rate: Union[float, Callable] = 0.1,\n",
        "                 weights: np.ndarray = None,\n",
        "                 metric: str = None,\n",
        "                 reg: str = None,\n",
        "                 l1_coef: float = 0.,\n",
        "                 l2_coef: float = 0.,\n",
        "                 sgd_sample: float = None,\n",
        "                 random_state: int = 42) -> None:\n",
        "\n",
        "        self.n_iter = n_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = weights\n",
        "        self.metric = metric\n",
        "        self.reg = reg\n",
        "        self.l1_coef = l1_coef\n",
        "        self.l2_coef = l2_coef\n",
        "        self.sgd_sample = sgd_sample\n",
        "        random.seed(random_state)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"MyLineReg class: n_iter={self.n_iter}, learning_rate={self.learning_rate}\"\n",
        "\n",
        "    def get_best_score(self):\n",
        "        return self.score\n",
        "\n",
        "    def get_coef(self):\n",
        "        return self.weights[1:]\n",
        "\n",
        "    def loss(self, y: pd.Series, y_pred: pd.Series) -> float:\n",
        "\n",
        "        loss = ((y - y_pred)**2).mean()\n",
        "\n",
        "        if self.reg == 'l1':\n",
        "            loss += self.l1_coef * np.abs(self.weights).sum()\n",
        "        elif self.reg == 'l2':\n",
        "            loss += self.l2_coef * np.square(self.weights).sum()\n",
        "        elif self.reg == 'elasticnet':\n",
        "            loss += self.l1_coef * np.abs(self.weights).sum() + self.l2_coef * np.square(self.weights).sum()\n",
        "        return loss\n",
        "\n",
        "    def predict(self, X):\n",
        "         X = pd.concat([pd.DataFrame([1] * X.shape[0], index=X.index), X], axis=1)\n",
        "         return np.dot(X, self.weights)\n",
        "\n",
        "    def get_batch(self, x_length):\n",
        "        return random.sample(range(x_length), int(self.sgd_sample * x_length)) if isinstance(self.sgd_sample, float) else random.sample(range(x_length), self.sgd_sample)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_metric_score(metric: str, y: pd.Series, y_pred: pd.Series) -> float:\n",
        "        \"\"\"\n",
        "        Вычисление метрики.\n",
        "\n",
        "        mae - средняя абсолютная ошибка\n",
        "        mse - среднеквадратичная ошибка\n",
        "        rmse - квадратный корень из среднеквадратичной ошибки\n",
        "        r2 - коэффициент детерминации\n",
        "        mape - средняя абсолютная ошибка в процента\n",
        "        \"\"\"\n",
        "\n",
        "        if metric == 'mae':\n",
        "            score = (y - y_pred).abs().mean()\n",
        "        elif metric == 'mse':\n",
        "            score = np.mean((y_pred - y)**2)\n",
        "        elif metric == 'rmse':\n",
        "            score = np.sqrt(np.mean((y_pred - y)**2))\n",
        "        elif metric == 'mape':\n",
        "            score = (100 / y.shape[0]) * np.sum(abs((y - y_pred) / y))\n",
        "        elif metric == 'r2':\n",
        "            score = 1 - ((np.sum((y - y_pred)**2)) / (np.sum((y - np.mean(y))**2)))\n",
        "        return score\n",
        "\n",
        "    def grad(self, y: pd.Series, y_pred: np.array, X: pd.DataFrame, batch_idxs: list) -> float:\n",
        "        y = y.iloc[batch_idxs]\n",
        "        y_pred = y_pred[batch_idxs]\n",
        "        X = X.iloc[batch_idxs]\n",
        "\n",
        "        if not self.reg:\n",
        "           grad = (2 / y.shape[0]) * ((y_pred - y)).dot(X)\n",
        "        elif self.reg == 'l1':\n",
        "           grad = (2 / y.shape[0]) * ((y_pred - y)).dot(X) + self.l1_coef * np.sign(self.weights)\n",
        "        elif self.reg == 'l2':\n",
        "           grad = (2 / y.shape[0]) * ((y_pred - y)).dot(X) + self.l2_coef * 2 * (self.weights)\n",
        "        elif self.reg == 'elasticnet':\n",
        "           grad = (2 / y.shape[0]) * ((y_pred - y)).dot(X) + self.l1_coef * np.sign(self.weights) + self.l2_coef * 2 * (self.weights)\n",
        "\n",
        "        return grad\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series, verbose: int = False) -> None:\n",
        "        \"\"\"Обучение линейной регрессии\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pd.DataFrame\n",
        "            Все фичи\n",
        "        y : pd.Series\n",
        "            Целевая переменная\n",
        "        verbose : int, optional\n",
        "            Указывает через сколько итераций градиентного спуска будет выводиться лог\n",
        "        \"\"\"\n",
        "        X = pd.concat([pd.DataFrame([1] * X.shape[0], index=X.index), X], axis=1)\n",
        "\n",
        "        self.weights = np.ones(X.shape[1])\n",
        "\n",
        "        for i in range(self.n_iter):\n",
        "            sample_rows_idx = self.get_batch(X.shape[0]) if self.sgd_sample else range(X.shape[0])\n",
        "            y_pred = np.dot(X, self.weights)\n",
        "            err = self.loss(y, y_pred)\n",
        "            nabla = self.grad(y, y_pred, X, sample_rows_idx)\n",
        "\n",
        "            if isinstance(self.learning_rate, (int, float)):\n",
        "                self.weights -= self.learning_rate * nabla\n",
        "            else:\n",
        "                self.weights -= self.learning_rate(i + 1) * nabla\n",
        "\n",
        "            if self.metric:\n",
        "               self.score = getattr(self, 'get_metric_score')(self.metric, y, np.dot(X, self.weights))\n",
        "\n",
        "            if verbose and i % verbose == 0:\n",
        "               if self.metric:\n",
        "                   print(f'start | loss = {err} | {self.score} | learning_rate = {self.learning_rate}') if i == 0 else print(f'i = {i} | loss = {err} | {self.score} | learning_rate = {self.learning_rate}')\n",
        "               else:\n",
        "                   print(f'start | loss = {err} | learning_rate = {self.learning_rate}') if i == 0 else print(f'i = {i} | loss = {err} | learning_rate = {self.learning_rate}')"
      ]
    }
  ]
}